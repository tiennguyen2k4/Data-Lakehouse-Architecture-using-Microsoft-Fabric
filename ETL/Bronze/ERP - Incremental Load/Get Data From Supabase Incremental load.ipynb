{"cells":[{"cell_type":"code","source":["import json\n","import requests\n","import pandas as pd\n","from datetime import datetime\n","from pyspark.sql import SparkSession\n","from pyspark.sql.types import (\n","    StructType, StructField, \n","    IntegerType, StringType, FloatType, DoubleType,\n","    DateType, TimestampType, BooleanType, DecimalType\n",")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"32f7e684-0d15-4c36-b820-b09f00416783"},{"cell_type":"code","source":["table_name = \"\"\n","schema_json = \"\"\n","primary_keys = \"\""],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"3cabc3b4-b728-4197-85a0-41b537ff11d1"},{"cell_type":"code","source":["print(\"=\" * 70)\n","print(\"INCREMENTAL LOAD WITH MERGE INTO\")\n","print(\"=\" * 70)\n","print(f\"\\n Parameters nhận được:\")\n","print(f\"  ├─ table_name: {table_name if table_name else '(empty)'}\")\n","print(f\"  └─ primary_keys: {primary_keys if primary_keys else '(empty)'}\")\n","\n","# Validate\n","if not table_name:\n","    raise ValueError(\"Thiếu parameter 'table_name'!\")\n","\n","if not schema_json:\n","    raise ValueError(\"Thiếu parameter 'schema_json'!\")\n","\n","if not primary_keys:\n","    raise ValueError(\"Thiếu parameter 'primary_keys' cho MERGE!\")\n","\n","# Parse primary keys\n","pk_list = [pk.strip() for pk in primary_keys.split(\",\")]"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"74010c3b-67f5-42af-893d-b55f37409439"},{"cell_type":"code","source":["def get_supabase_credentials():\n","    \"\"\"Load Supabase credentials từ Lakehouse\"\"\"\n","    config_path = \"/lakehouse/default/Files/.config/credentials.json\"\n","    \n","    try:\n","        with open(config_path, \"r\") as f:\n","            creds = json.load(f)\n","        return creds[\"supabase\"][\"url\"], creds[\"supabase\"][\"key\"]\n","    except FileNotFoundError:\n","        print(\"Chưa setup credentials!\")\n","        return None, None\n","    except Exception as e:\n","        print(f\"Lỗi đọc config: {e}\")\n","        return None, None"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e67b743d-1386-46a3-b4ab-ca67bcf7103c"},{"cell_type":"code","source":["def parse_schema_from_json(schema_json_str: str) -> StructType:\n","    \"\"\"Parse schema từ JSON string\"\"\"\n","    try:\n","        schema_dict = json.loads(schema_json_str)\n","        \n","        type_mapping = {\n","            \"IntegerType\": IntegerType(),\n","            \"StringType\": StringType(),\n","            \"FloatType\": FloatType(),\n","            \"DoubleType\": DoubleType(),\n","            \"DateType\": DateType(),\n","            \"TimestampType\": TimestampType(),\n","            \"BooleanType\": BooleanType(),\n","        }\n","        \n","        fields = []\n","        for field_dict in schema_dict[\"fields\"]:\n","            field_name = field_dict[\"name\"]\n","            field_type_str = field_dict[\"type\"]\n","            nullable = field_dict.get(\"nullable\", True)\n","            \n","            if field_type_str.startswith(\"DecimalType\"):\n","                import re\n","                match = re.search(r'DecimalType\\((\\d+),\\s*(\\d+)\\)', field_type_str)\n","                if match:\n","                    precision, scale = int(match.group(1)), int(match.group(2))\n","                    field_type = DecimalType(precision, scale)\n","                else:\n","                    field_type = DecimalType(19, 4)\n","            else:\n","                field_type = type_mapping.get(field_type_str, StringType())\n","            \n","            fields.append(StructField(field_name, field_type, nullable))\n","        \n","        return StructType(fields)\n","        \n","    except Exception as e:\n","        print(f\"Lỗi parse schema: {e}\")\n","        raise"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"32529c97-c70a-45ac-bbad-b8d38bb509a6"},{"cell_type":"code","source":["def get_last_watermark(spark, table_name: str):\n","    \"\"\"Lấy max_modified_date từ watermark table\"\"\"\n","    try:\n","        watermark_table = \"water_mark\"\n","        \n","        # Kiểm tra xem bảng watermark có tồn tại không\n","        if not spark.catalog.tableExists(watermark_table):\n","            print(f\"Bảng watermark chưa tồn tại, sẽ load full data lần đầu\")\n","            return None\n","        \n","        # Lấy watermark\n","        result = spark.sql(f\"\"\"\n","            SELECT max_modified_date \n","            FROM {watermark_table}\n","            WHERE table_name = '{table_name}'\n","        \"\"\").collect()\n","        \n","        if result and result[0][0] is not None:\n","            watermark_date = result[0][0]\n","            print(f\"Tìm thấy watermark: {watermark_date}\")\n","            return watermark_date\n","        else:\n","            print(f\"Chưa có watermark cho bảng {table_name}, sẽ load full data\")\n","            return None\n","            \n","    except Exception as e:\n","        print(f\"Lỗi khi lấy watermark: {e}\")\n","        return None"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"785050b8-e62e-44ed-b5e0-618f803fdf0f"},{"cell_type":"code","source":["def fetch_incremental_data(base_url: str, api_key: str, table_name: str, \n","                          last_modified_date=None, batch_size: int = 1000) -> pd.DataFrame:\n","    \"\"\"Lấy dữ liệu incremental từ Supabase (ModifiedDate > watermark)\"\"\"\n","    headers = {\n","        \"apikey\": api_key,\n","        \"Authorization\": f\"Bearer {api_key}\",\n","        \"Content-Type\": \"application/json\"\n","    }\n","    \n","    all_data = []\n","    offset = 0\n","    \n","    print(f\"\\nĐang lấy dữ liệu từ bảng: {table_name}\")\n","    \n","    if last_modified_date:\n","        # Convert datetime to ISO format string cho Supabase filter\n","        if isinstance(last_modified_date, datetime):\n","            last_modified_str = last_modified_date.isoformat()\n","        else:\n","            last_modified_str = str(last_modified_date)\n","        \n","        print(f\"  ├─ Filter: ModifiedDate > {last_modified_str}\")\n","    else:\n","        print(f\"  ├─ Mode: FULL LOAD (lần đầu)\")\n","    \n","    while True:\n","        url = f\"{base_url}/rest/v1/{table_name}\"\n","        \n","        params = {\n","            \"select\": \"*\",\n","            \"limit\": batch_size,\n","            \"offset\": offset\n","        }\n","        \n","        # Thêm filter nếu có watermark\n","        if last_modified_date:\n","            params[\"ModifiedDate\"] = f\"gt.{last_modified_str}\"\n","            params[\"order\"] = \"ModifiedDate.asc\"\n","        \n","        try:\n","            response = requests.get(url, headers=headers, params=params)\n","            response.raise_for_status()\n","            \n","            data = response.json()\n","            \n","            if not data:\n","                break\n","            \n","            all_data.extend(data)\n","            print(f\"  ├─ Đã lấy {len(all_data)} records...\")\n","            \n","            if len(data) < batch_size:\n","                break\n","            \n","            offset += batch_size\n","            \n","        except requests.exceptions.RequestException as e:\n","            print(f\"Lỗi request: {e}\")\n","            return pd.DataFrame()\n","    \n","    if all_data:\n","        df = pd.DataFrame(all_data)\n","        print(f\"Hoàn tất: {len(df)} records x {len(df.columns)} columns\")\n","        return df\n","    else:\n","        print(f\"Không có dữ liệu mới\")\n","        return pd.DataFrame()"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"21c0da14-3141-4818-950c-b5dc73bba0d3"},{"cell_type":"code","source":["def create_watermark_table_if_not_exists(spark):\n","    \"\"\"Tạo bảng watermark nếu chưa tồn tại\"\"\"\n","    try:\n","        watermark_table = \"water_mark\"\n","        \n","        # Kiểm tra bảng có tồn tại không\n","        tables = [t.name for t in spark.catalog.listTables()]\n","        \n","        if watermark_table not in tables:\n","            print(f\"\\n Tạo bảng watermark...\")\n","            \n","            create_query = f\"\"\"\n","            CREATE TABLE IF NOT EXISTS {watermark_table} (\n","                table_name STRING,\n","                max_modified_date TIMESTAMP,\n","                layer_type STRING,\n","                last_load_time TIMESTAMP,\n","                row_count BIGINT\n","            ) USING DELTA\n","            \"\"\"\n","            spark.sql(create_query)\n","            print(f\"Đã tạo bảng {watermark_table}\")\n","            \n","    except Exception as e:\n","        print(f\"Lỗi khi tạo bảng watermark: {e}\")\n","        raise"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c4fbd959-c380-4fb4-91b7-1ab85d83d0e1"},{"cell_type":"code","source":["def update_watermark(spark, table_name: str, pandas_df: pd.DataFrame, \n","                     layer_type: str = \"bronze\"):\n","    \"\"\"Cập nhật watermark cho 1 bảng\"\"\"\n","    try:\n","        create_watermark_table_if_not_exists(spark)\n","        \n","        watermark_table = \"water_mark\"\n","        \n","        # Tìm max_modified_date\n","        if 'ModifiedDate' in pandas_df.columns:\n","            pandas_df['ModifiedDate'] = pd.to_datetime(pandas_df['ModifiedDate'])\n","            max_modified = pandas_df['ModifiedDate'].max()\n","            \n","            if pd.notna(max_modified):\n","                max_modified = max_modified.to_pydatetime()\n","            else:\n","                max_modified = datetime.now()\n","                print(f\"ModifiedDate có giá trị NULL, dùng datetime.now()\")\n","        else:\n","            print(f\"Không có cột ModifiedDate, dùng datetime.now()\")\n","            max_modified = datetime.now()\n","        \n","        # Tạo record watermark\n","        watermark_record = [{\n","            'table_name': table_name,\n","            'max_modified_date': max_modified,\n","            'layer_type': layer_type,\n","            'last_load_time': datetime.now(),\n","            'row_count': len(pandas_df)\n","        }]\n","        \n","        watermark_df = spark.createDataFrame(watermark_record)\n","        watermark_df.createOrReplaceTempView(\"temp_watermark\")\n","        \n","        merge_query = f\"\"\"\n","        MERGE INTO {watermark_table} AS target\n","        USING temp_watermark AS source\n","        ON target.table_name = source.table_name\n","        WHEN MATCHED THEN\n","            UPDATE SET \n","                max_modified_date = source.max_modified_date,\n","                last_load_time = source.last_load_time,\n","                layer_type = source.layer_type,\n","                row_count = source.row_count\n","        WHEN NOT MATCHED THEN\n","            INSERT (table_name, max_modified_date, layer_type, last_load_time, row_count)\n","            VALUES (source.table_name, source.max_modified_date, source.layer_type, \n","                    source.last_load_time, source.row_count)\n","        \"\"\"\n","        \n","        spark.sql(merge_query)\n","        \n","        print(f\"\\nĐã cập nhật watermark:\")\n","        print(f\"  ├─ Table: {table_name}\")\n","        print(f\"  ├─ Max Modified Date: {max_modified}\")\n","        print(f\"  ├─ Row Count: {len(pandas_df):,}\")\n","        print(f\"  └─ Load Time: {datetime.now()}\")\n","        \n","    except Exception as e:\n","        print(f\"\\nLỗi khi cập nhật watermark: {e}\")\n","        raise"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"78afbc1d-2a36-448b-8b70-cb4f857a2748"},{"cell_type":"code","source":["def merge_data_into_table(spark, source_df, target_table: str, primary_keys: list):\n","    \"\"\"MERGE dữ liệu vào target table\"\"\"\n","    try:\n","        # Tạo temp view cho source data\n","        source_df.createOrReplaceTempView(\"temp_source\")\n","        \n","        # Build JOIN condition từ primary keys\n","        join_conditions = \" AND \".join([\n","            f\"target.{pk} = source.{pk}\" for pk in primary_keys\n","        ])\n","        \n","        # Build UPDATE SET clause (tất cả columns trừ primary keys)\n","        all_columns = source_df.columns\n","        update_columns = [col for col in all_columns if col not in primary_keys]\n","        update_set = \", \".join([f\"{col} = source.{col}\" for col in update_columns])\n","        \n","        # Build INSERT clause\n","        insert_columns = \", \".join(all_columns)\n","        insert_values = \", \".join([f\"source.{col}\" for col in all_columns])\n","        \n","        # MERGE query\n","        merge_query = f\"\"\"\n","        MERGE INTO {target_table} AS target\n","        USING temp_source AS source\n","        ON {join_conditions}\n","        WHEN MATCHED THEN\n","            UPDATE SET {update_set}\n","        WHEN NOT MATCHED THEN\n","            INSERT ({insert_columns})\n","            VALUES ({insert_values})\n","        \"\"\"\n","        \n","        print(f\"\\n Thực hiện MERGE INTO...\")\n","        print(f\"  ├─ Target: {target_table}\")\n","        print(f\"  ├─ Primary Keys: {', '.join(primary_keys)}\")\n","        print(f\"  └─ Source Records: {source_df.count():,}\")\n","        \n","        spark.sql(merge_query)\n","        \n","        print(f\"MERGE hoàn tất!\")\n","        \n","    except Exception as e:\n","        print(f\"Lỗi khi MERGE: {e}\")\n","        raise"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0a81371b-08df-490d-a971-05e34c1940e6"},{"cell_type":"code","source":["print(\"\\n\" + \"=\" * 70)\n","print(f\"INCREMENTAL LOAD TABLE: {table_name}\")\n","print(\"=\" * 70)\n","\n","try:\n","    # Bước 1: Parse schema\n","    print(f\"\\n Parse schema...\")\n","    schema = parse_schema_from_json(schema_json)\n","    print(f\"Schema có {len(schema.fields)} cột\")\n","    \n","    # Bước 2: Khởi tạo Spark\n","    spark = SparkSession.builder.appName(f\"IncrementalLoad_{table_name}\").getOrCreate()\n","    \n","    full_table_name = f\"{table_name}\"\n","    \n","    # Bước 3: Lấy watermark\n","    last_watermark = get_last_watermark(spark, table_name)\n","    \n","    # Bước 4: Lấy dữ liệu incremental từ Supabase\n","    url, key = get_supabase_credentials()\n","    if not url or not key:\n","        raise ValueError(\"Không thể lấy credentials!\")\n","    \n","    pandas_df = fetch_incremental_data(url, key, table_name, last_watermark)\n","    \n","    # Nếu không có dữ liệu mới\n","    # Nếu không có dữ liệu mới\n","    if pandas_df.empty:\n","        print(\"\\n Không có dữ liệu mới để load!\")\n","        print(f\"  └─ Watermark hiện tại: {last_watermark}\")\n","        \n","        # Không thoát với lỗi nữa — chỉ in thông báo và kết thúc bình thường\n","        print(\"\\n Bỏ qua tác vụ vì không có dữ liệu mới.\")\n","        \n","        mssparkutils.notebook.exit(\"SUCCESS_NO_NEW_DATA\")\n","\n","    \n","    # Bước 5: Convert sang Spark DataFrame với schema\n","    print(f\"\\n Chuyển đổi DataFrame với schema...\")\n","    try:\n","        # Convert các cột datetime trong pandas trước\n","        for field in schema.fields:\n","            if isinstance(field.dataType, TimestampType) and field.name in pandas_df.columns:\n","                pandas_df[field.name] = pd.to_datetime(pandas_df[field.name], errors='coerce')\n","        \n","        spark_df = spark.createDataFrame(pandas_df, schema=schema)\n","        print(f\"Đã áp dụng schema\")\n","        \n","    except Exception as e:\n","        print(f\"Lỗi khi áp dụng schema: {e}\")\n","        print(\"Đang thử load mà không ép schema...\")\n","        spark_df = spark.createDataFrame(pandas_df)\n","    \n","    # Bước 6: Kiểm tra target table có tồn tại chưa\n","    if not spark.catalog.tableExists(full_table_name):\n","        print(f\"\\n Bảng {full_table_name} chưa tồn tại, tạo mới...\")\n","        \n","        spark_df.write \\\n","            .mode(\"overwrite\") \\\n","            .format(\"delta\") \\\n","            .saveAsTable(full_table_name)\n","        \n","        print(f\"Đã tạo bảng {full_table_name}\")\n","    else:\n","        # Bước 7: MERGE dữ liệu vào target table\n","        merge_data_into_table(spark, spark_df, full_table_name, pk_list)\n","    \n","    # Bước 8: Cập nhật watermark\n","    print(f\"\\nCập nhật watermark...\")\n","    update_watermark(spark, table_name, pandas_df)\n","    \n","    # Bước 9: Hiển thị kết quả\n","    print(\"\\n\" + \"=\" * 70)\n","    print(\"HOÀN TẤT INCREMENTAL LOAD\")\n","    print(\"=\" * 70)\n","    \n","    row_count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {full_table_name}\").collect()[0][0]\n","    \n","    print(f\"\\nThống kê:\")\n","    print(f\"  ├─ Table: {full_table_name}\")\n","    print(f\"  ├─ Total Rows: {row_count:,}\")\n","    print(f\"  ├─ New/Updated Rows: {len(pandas_df):,}\")\n","    print(f\"  └─ Load time: {datetime.now()}\")\n","    \n","    # Hiển thị watermark\n","    print(f\"\\n Watermark hiện tại:\")\n","    spark.sql(f\"\"\"\n","        SELECT * FROM water_mark \n","        WHERE table_name = '{table_name}'\n","    \"\"\").show(truncate=False)\n","    \n","    # Return success\n","    mssparkutils.notebook.exit(\"SUCCESS\")\n","    \n","except Exception as e:\n","    error_msg = f\"FAILED: {str(e)}\"\n","    print(f\"\\n LỖI: {error_msg}\")\n","    import traceback\n","    traceback.print_exc()\n","    mssparkutils.notebook.exit(error_msg)\n","    raise\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9d96fb23-20b9-47fc-9122-eb61fd1baf64"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"524bca3f-de2b-4efc-ac4d-a55e989a0834"}],"default_lakehouse":"524bca3f-de2b-4efc-ac4d-a55e989a0834","default_lakehouse_name":"DATN_Bronze_LH","default_lakehouse_workspace_id":"5fd7fd53-1561-487d-970f-160a2ad7dc71"}}},"nbformat":4,"nbformat_minor":5}