{"cells":[{"cell_type":"code","source":["import json\n","import requests\n","import pandas as pd\n","from datetime import datetime\n","from pyspark.sql import SparkSession\n","from pyspark.sql.types import (\n","    StructType, StructField, \n","    IntegerType, StringType, FloatType, DoubleType,\n","    DateType, TimestampType, BooleanType, DecimalType\n",")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"58cd9ec5-d5fb-48b6-9a45-a2db7c25b47d"},{"cell_type":"code","source":["table_name = \"\"\n","schema_json = \"\"\n","drop_before_load = \"true\""],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"17850232-99e9-4ac2-9c42-1e660e90f21a"},{"cell_type":"code","source":["print(\"=\" * 70)\n","print(\"FULL LOAD TABLE \")\n","print(\"=\" * 70)\n","print(f\"\\nParameters nháº­n Ä‘Æ°á»£c:\")\n","print(f\"  â”œâ”€ table_name: {table_name if table_name else '(empty)'}\")\n","print(f\"  â””â”€ drop_before_load: {drop_before_load}\")\n","\n","# Validate\n","if not table_name:\n","    raise ValueError(\"Thiáº¿u parameter 'table_name'!\")\n","\n","if not schema_json:\n","    raise ValueError(\"Thiáº¿u parameter 'schema_json'!\")\n","\n","# Convert drop_before_load to boolean\n","drop_before_load_bool = drop_before_load.lower() == \"true\" if isinstance(drop_before_load, str) else drop_before_load\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"04b6565f-addf-4246-a52e-1d12284a5062"},{"cell_type":"code","source":["def get_supabase_credentials():\n","    \"\"\"Load Supabase credentials tá»« Lakehouse\"\"\"\n","    config_path = \"/lakehouse/default/Files/.config/credentials.json\"\n","    \n","    try:\n","        with open(config_path, \"r\") as f:\n","            creds = json.load(f)\n","        return creds[\"supabase\"][\"url\"], creds[\"supabase\"][\"key\"]\n","    except FileNotFoundError:\n","        print(\"ChÆ°a setup credentials!\")\n","        return None, None\n","    except Exception as e:\n","        print(f\"Lá»—i Ä‘á»c config: {e}\")\n","        return None, None\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"487c1bb7-7d9e-423e-91b5-76ea5b794734"},{"cell_type":"code","source":["def parse_schema_from_json(schema_json_str: str) -> StructType:\n","    \"\"\"Parse schema tá»« JSON string\"\"\"\n","    try:\n","        schema_dict = json.loads(schema_json_str)\n","        \n","        type_mapping = {\n","            \"IntegerType\": IntegerType(),\n","            \"StringType\": StringType(),\n","            \"FloatType\": FloatType(),\n","            \"DoubleType\": DoubleType(),\n","            \"DateType\": DateType(),\n","            \"TimestampType\": TimestampType(),\n","            \"BooleanType\": BooleanType(),\n","            \"DecimalType\": lambda p, s: DecimalType(p, s)\n","        }\n","        \n","        fields = []\n","        for field_dict in schema_dict[\"fields\"]:\n","            field_name = field_dict[\"name\"]\n","            field_type_str = field_dict[\"type\"]\n","            nullable = field_dict.get(\"nullable\", True)\n","            \n","            # Xá»­ lÃ½ DecimalType\n","            if field_type_str.startswith(\"DecimalType\"):\n","                import re\n","                match = re.search(r'DecimalType\\((\\d+),\\s*(\\d+)\\)', field_type_str)\n","                if match:\n","                    precision, scale = int(match.group(1)), int(match.group(2))\n","                    field_type = DecimalType(precision, scale)\n","                else:\n","                    field_type = DecimalType(19, 4)  # Default\n","            else:\n","                field_type = type_mapping.get(field_type_str, StringType())\n","            \n","            fields.append(StructField(field_name, field_type, nullable))\n","        \n","        return StructType(fields)\n","        \n","    except Exception as e:\n","        print(f\"Lá»—i parse schema: {e}\")\n","        raise"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"39236d1e-8e52-4baf-8cdc-5967a7291df8"},{"cell_type":"code","source":["def fetch_table_data(base_url: str, api_key: str, table_name: str, \n","                     batch_size: int = 1000) -> pd.DataFrame:\n","    \"\"\"Láº¥y dá»¯ liá»‡u tá»« 1 báº£ng Supabase qua REST API\"\"\"\n","    headers = {\n","        \"apikey\": api_key,\n","        \"Authorization\": f\"Bearer {api_key}\",\n","        \"Content-Type\": \"application/json\"\n","    }\n","    \n","    all_data = []\n","    offset = 0\n","    \n","    print(f\"\\nÄang láº¥y dá»¯ liá»‡u tá»« báº£ng: {table_name}\")\n","    \n","    while True:\n","        url = f\"{base_url}/rest/v1/{table_name}\"\n","        \n","        params = {\n","            \"select\": \"*\",\n","            \"limit\": batch_size,\n","            \"offset\": offset\n","        }\n","        \n","        try:\n","            response = requests.get(url, headers=headers, params=params)\n","            response.raise_for_status()\n","            \n","            data = response.json()\n","            \n","            if not data:\n","                break\n","            \n","            all_data.extend(data)\n","            print(f\"  â”œâ”€ ÄÃ£ láº¥y {len(all_data)} records...\")\n","            \n","            if len(data) < batch_size:\n","                break\n","            \n","            offset += batch_size\n","            \n","        except requests.exceptions.RequestException as e:\n","            print(f\"Lá»—i request: {e}\")\n","            return pd.DataFrame()\n","    \n","    if all_data:\n","        df = pd.DataFrame(all_data)\n","        print(f\"HoÃ n táº¥t: {len(df)} records x {len(df.columns)} columns\")\n","        return df\n","    else:\n","        print(f\"KhÃ´ng cÃ³ dá»¯ liá»‡u\")\n","        return pd.DataFrame()"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"59616ff7-9380-484a-9329-63e3a0414f73"},{"cell_type":"code","source":["def create_watermark_table_if_not_exists(spark):\n","    \"\"\"Táº¡o báº£ng watermark náº¿u chÆ°a tá»“n táº¡i\"\"\"\n","    try:\n","        watermark_table = \"water_mark\"\n","        \n","        # Kiá»ƒm tra báº£ng cÃ³ tá»“n táº¡i khÃ´ng\n","        tables = [t.name for t in spark.catalog.listTables()]\n","        \n","        if watermark_table not in tables:\n","            print(f\"\\nTáº¡o báº£ng watermark...\")\n","            \n","            create_query = f\"\"\"\n","            CREATE TABLE IF NOT EXISTS {watermark_table} (\n","                table_name STRING,\n","                max_modified_date TIMESTAMP,\n","                layer_type STRING,\n","                last_load_time TIMESTAMP,\n","                row_count BIGINT\n","            ) USING DELTA\n","            \"\"\"\n","            spark.sql(create_query)\n","            print(f\"ÄÃ£ táº¡o báº£ng {watermark_table}\")\n","            \n","    except Exception as e:\n","        print(f\"Lá»—i khi táº¡o báº£ng watermark: {e}\")\n","        raise"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8073ed82-e13a-4797-b970-dbc420f2b63b"},{"cell_type":"code","source":["def update_watermark(spark, table_name: str, pandas_df: pd.DataFrame, \n","                     layer_type: str = \"bronze\"):\n","    \"\"\"Cáº­p nháº­t watermark cho 1 báº£ng\"\"\"\n","    try:\n","        create_watermark_table_if_not_exists(spark)\n","        \n","        watermark_table = \"water_mark\"\n","        \n","        # TÃ¬m max_modified_date\n","        if 'ModifiedDate' in pandas_df.columns:\n","            pandas_df['ModifiedDate'] = pd.to_datetime(pandas_df['ModifiedDate'])\n","            max_modified = pandas_df['ModifiedDate'].max()\n","            \n","            if pd.notna(max_modified):\n","                max_modified = max_modified.to_pydatetime()\n","            else:\n","                max_modified = datetime.now()\n","                print(f\"ModifiedDate cÃ³ giÃ¡ trá»‹ NULL, dÃ¹ng datetime.now()\")\n","        else:\n","            print(f\"KhÃ´ng cÃ³ cá»™t ModifiedDate, dÃ¹ng datetime.now()\")\n","            max_modified = datetime.now()\n","        \n","        # Táº¡o record watermark\n","        watermark_record = [{\n","            'table_name': table_name,\n","            'max_modified_date': max_modified,\n","            'layer_type': layer_type,\n","            'last_load_time': datetime.now(),\n","            'row_count': len(pandas_df)\n","        }]\n","        \n","        watermark_df = spark.createDataFrame(watermark_record)\n","        watermark_df.createOrReplaceTempView(\"temp_watermark\")\n","        \n","        merge_query = f\"\"\"\n","        MERGE INTO {watermark_table} AS target\n","        USING temp_watermark AS source\n","        ON target.table_name = source.table_name\n","        WHEN MATCHED THEN\n","            UPDATE SET \n","                max_modified_date = source.max_modified_date,\n","                last_load_time = source.last_load_time,\n","                layer_type = source.layer_type,\n","                row_count = source.row_count\n","        WHEN NOT MATCHED THEN\n","            INSERT (table_name, max_modified_date, layer_type, last_load_time, row_count)\n","            VALUES (source.table_name, source.max_modified_date, source.layer_type, \n","                    source.last_load_time, source.row_count)\n","        \"\"\"\n","        \n","        spark.sql(merge_query)\n","        \n","        print(f\"\\nÄÃ£ cáº­p nháº­t watermark:\")\n","        print(f\"  â”œâ”€ Table: {table_name}\")\n","        print(f\"  â”œâ”€ Max Modified Date: {max_modified}\")\n","        print(f\"  â”œâ”€ Row Count: {len(pandas_df):,}\")\n","        print(f\"  â””â”€ Load Time: {datetime.now()}\")\n","        \n","    except Exception as e:\n","        print(f\"\\nLá»—i khi cáº­p nháº­t watermark: {e}\")\n","        raise"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9b1835d7-fcec-49e7-b3f1-67e48d7405eb"},{"cell_type":"code","source":["print(\"\\n\" + \"=\" * 70)\n","print(f\"LOAD TABLE: {table_name}\")\n","print(\"=\" * 70)\n","\n","try:\n","    # BÆ°á»›c 1: Parse schema\n","    print(f\"\\nParse schema...\")\n","    schema = parse_schema_from_json(schema_json)\n","    print(f\"Schema cÃ³ {len(schema.fields)} cá»™t\")\n","    \n","    # BÆ°á»›c 2: Láº¥y dá»¯ liá»‡u tá»« Supabase\n","    url, key = get_supabase_credentials()\n","    if not url or not key:\n","        raise ValueError(\"KhÃ´ng thá»ƒ láº¥y credentials!\")\n","    \n","    pandas_df = fetch_table_data(url, key, table_name)\n","    \n","    if pandas_df.empty:\n","        raise ValueError(\"KhÃ´ng cÃ³ dá»¯ liá»‡u Ä‘á»ƒ load!\")\n","    \n","    # BÆ°á»›c 3: Khá»Ÿi táº¡o Spark\n","    spark = SparkSession.builder.appName(f\"Load_{table_name}\").getOrCreate()\n","    \n","    # BÆ°á»›c 4: Convert sang Spark DataFrame vá»›i schema\n","    print(f\"\\nChuyá»ƒn Ä‘á»•i DataFrame vá»›i schema...\")\n","    try:\n","        # # Convert cÃ¡c cá»™t datetime trong pandas trÆ°á»›c\n","        # for field in schema.fields:\n","        #     if isinstance(field.dataType, TimestampType) and field.name in pandas_df.columns:\n","        #         pandas_df[field.name] = pd.to_datetime(pandas_df[field.name], errors='coerce')\n","        print(f\"  â”œâ”€ Äang convert timestamp columns...\")\n","        for field in schema.fields:\n","            if field.name in pandas_df.columns:\n","                if isinstance(field.dataType, TimestampType):\n","                    print(f\"    â”œâ”€ {field.name}: String â†’ Timestamp\")\n","                    pandas_df[field.name] = pd.to_datetime(pandas_df[field.name], errors='coerce', utc=True)\n","                    # Remove timezone Ä‘á»ƒ trÃ¡nh lá»—i vá»›i Spark\n","                    pandas_df[field.name] = pandas_df[field.name].dt.tz_localize(None)\n","                elif isinstance(field.dataType, DateType):\n","                    print(f\"    â”œâ”€ {field.name}: String â†’ Date\")\n","                    pandas_df[field.name] = pd.to_datetime(pandas_df[field.name], errors='coerce').dt.date\n","        \n","        # Show sample data sau khi convert\n","        print(f\"\\n  ğŸ“‹ Sample data sau convert:\")\n","        print(f\"    {pandas_df.dtypes.to_dict()}\")\n","        \n","        spark_df = spark.createDataFrame(pandas_df, schema=schema)\n","        print(f\"ÄÃ£ Ã¡p dá»¥ng schema\")\n","        \n","        # Hiá»ƒn thá»‹ schema\n","        print(f\"\\nSchema cá»§a báº£ng:\")\n","        spark_df.printSchema()\n","        \n","    except Exception as e:\n","        print(f\"Lá»—i khi Ã¡p dá»¥ng schema: {e}\")\n","        print(\"Äang thá»­ load mÃ  khÃ´ng Ã©p schema...\")\n","        spark_df = spark.createDataFrame(pandas_df)\n","    \n","    # BÆ°á»›c 5: Ghi vÃ o Lakehouse (khÃ´ng dÃ¹ng schema prefix)\n","    print(f\"\\nGhi dá»¯ liá»‡u vÃ o: {table_name}\")\n","    \n","    # DROP TABLE náº¿u cáº§n\n","    if drop_before_load_bool and spark.catalog.tableExists(table_name):\n","        print(f\"  â”œâ”€ DROP TABLE {table_name}\")\n","        spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n","    \n","    # Ghi vÃ o báº£ng Delta\n","    spark_df.write \\\n","        .mode(\"overwrite\") \\\n","        .option(\"mergeSchema\", \"true\") \\\n","        .format(\"delta\") \\\n","        .saveAsTable(table_name)\n","    \n","    row_count = spark_df.count()\n","    print(f\"ÄÃ£ ghi {row_count:,} rows vÃ o {table_name}\")\n","    \n","    # BÆ°á»›c 6: Cáº­p nháº­t watermark\n","    print(f\"\\nCáº­p nháº­t watermark...\")\n","    update_watermark(spark, table_name, pandas_df)\n","    \n","    # BÆ°á»›c 7: Hiá»ƒn thá»‹ káº¿t quáº£\n","    print(\"\\n\" + \"=\" * 70)\n","    print(\"HOÃ€N Táº¤T LOAD TABLE\")\n","    print(\"=\" * 70)\n","    \n","    print(f\"\\nThá»‘ng kÃª:\")\n","    print(f\"  â”œâ”€ Table: {table_name}\")\n","    print(f\"  â”œâ”€ Rows: {row_count:,}\")\n","    print(f\"  â”œâ”€ Columns: {len(spark_df.columns)}\")\n","    print(f\"  â””â”€ Load time: {datetime.now()}\")\n","    \n","    # Hiá»ƒn thá»‹ 5 dÃ²ng Ä‘áº§u\n","    print(f\"\\nPreview 5 dÃ²ng Ä‘áº§u:\")\n","    spark.sql(f\"SELECT * FROM {table_name} LIMIT 5\").show(5, truncate=False)\n","    \n","    # Hiá»ƒn thá»‹ watermark\n","    print(f\"\\nWatermark hiá»‡n táº¡i:\")\n","    spark.sql(f\"\"\"\n","        SELECT * FROM water_mark \n","        WHERE table_name = '{table_name}'\n","    \"\"\").show(truncate=False)\n","    \n","    # Exit vá»›i SUCCESS (chá»‰ 1 láº§n duy nháº¥t)\n","    print(\"\\nEXIT: SUCCESS\")\n","    mssparkutils.notebook.exit(\"SUCCESS\")\n","    \n","except Exception as e:\n","    # Exit vá»›i FAILED (chá»‰ 1 láº§n duy nháº¥t)\n","    error_msg = f\"FAILED: {str(e)}\"\n","    print(f\"\\nLá»–I: {error_msg}\")\n","    import traceback\n","    traceback.print_exc()\n","    print(f\"\\nEXIT: {error_msg}\")\n","    mssparkutils.notebook.exit(error_msg)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e283caaf-23be-4010-902b-23b69709a175"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"524bca3f-de2b-4efc-ac4d-a55e989a0834"}],"default_lakehouse":"524bca3f-de2b-4efc-ac4d-a55e989a0834","default_lakehouse_name":"DATN_Bronze_LH","default_lakehouse_workspace_id":"5fd7fd53-1561-487d-970f-160a2ad7dc71"}}},"nbformat":4,"nbformat_minor":5}